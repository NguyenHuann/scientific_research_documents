# Truy xuất hình ảnh dựa trên nội dung và khoảng cách ngữ nghĩa trong kỷ nguyên học sâu

**Björn Barz và Joachim Denzler**  
Nhóm Thị giác Máy tính, Đại học Friedrich Schiller Jena, Jena, Đức  
{bjoern.barz, joachim.denzler}@uni-jena.de

## Tóm tắt
Truy xuất hình ảnh dựa trên nội dung đã đạt được những tiến bộ đáng kinh ngạc trong thập kỷ qua, đặc biệt là đối với nhiệm vụ truy xuất hình ảnh của cùng một đối tượng được thể hiện trong ảnh truy vấn. Kịch bản này được gọi là truy xuất thể hiện hoặc truy xuất đối tượng và yêu cầu đối chiếu các mẫu hình ảnh chi tiết giữa các hình ảnh. Tuy nhiên, yếu tố ngữ nghĩa không đóng vai trò quan trọng.

Điều này đặt ra câu hỏi: Liệu những tiến bộ gần đây trong truy xuất thể hiện có thể chuyển giao sang các kịch bản truy xuất hình ảnh tổng quát hơn không?

Để trả lời câu hỏi này, trước tiên chúng tôi cung cấp một cái nhìn tổng quan ngắn gọn về những cột mốc quan trọng nhất của truy xuất thể hiện. Sau đó, chúng tôi áp dụng các phương pháp này vào một nhiệm vụ truy xuất hình ảnh ngữ nghĩa và nhận thấy rằng chúng hoạt động kém hiệu quả hơn nhiều so với các phương pháp ít phức tạp và mang tính tổng quát hơn trong bối cảnh yêu cầu hiểu hình ảnh. Tiếp theo, chúng tôi xem xét các phương pháp hiện có nhằm thu hẹp cái gọi là "khoảng cách ngữ nghĩa" bằng cách tích hợp kiến thức thế giới có sẵn.

Chúng tôi kết luận rằng vấn đề then chốt đối với sự tiến bộ hơn nữa của truy xuất hình ảnh ngữ nghĩa nằm ở việc thiếu một định nghĩa nhiệm vụ tiêu chuẩn hóa và bộ dữ liệu đánh giá thích hợp.

**Từ khóa:** Truy xuất hình ảnh dựa trên nội dung · Truy xuất thể hiện · Truy xuất đối tượng · Truy xuất hình ảnh ngữ nghĩa · Khoảng cách ngữ nghĩa
## 1. Giới thiệu

*"Người ta chỉ có thể nhìn rõ bằng trái tim. Điều cốt yếu thì vô hình đối với đôi mắt."*  
Câu nói nổi tiếng này của nhà văn Pháp **Antoine de Saint-Exupéry** áp dụng không chỉ cho cuộc sống mà còn cho cả lĩnh vực **thị giác máy tính**. Nhận thức của con người về hình ảnh vượt xa bề mặt thị giác bao gồm các điểm ảnh, màu sắc và các đối tượng. Ý nghĩa của một hình ảnh không thể chỉ được mô tả đơn giản bằng cách liệt kê tất cả các đối tượng có trong đó và xác định bố cục không gian của chúng.  

Chúng ta, với tư cách là con người, có khả năng nắm bắt vô vàn thông tin đa dạng và phức tạp chứa trong một hình ảnh chỉ trong nháy mắt, chẳng hạn như **các sự kiện đang xảy ra trong cảnh được thể hiện**, **các hoạt động mà con người đang thực hiện**, **mối quan hệ giữa họ**, **bầu không khí và tâm trạng của hình ảnh**, cũng như **cảm xúc mà nó truyền tải**. Nhiều khái niệm trong số này khó có thể mô tả bằng văn bản và thường được minh họa tốt nhất bằng cách cung cấp một hình ảnh ví dụ.

Ví dụ trong **Hình 1** minh họa sự đa dạng thông tin mà hình ảnh có thể truyền tải. Hình ảnh này có thể được mô tả từ nhiều góc độ khác nhau: **nội dung ngữ nghĩa**, **phong cách nghệ thuật**, **cảm xúc mà nó gợi lên cho người quan sát**, hoặc **thông tin siêu dữ liệu** về chính hình ảnh.  

Tùy thuộc vào **nền tảng** và **bối cảnh tình huống**, mỗi người quan sát sẽ nhận thức và diễn giải hình ảnh này theo một cách khác nhau. Do đó, việc **tìm kiếm hình ảnh trên web bằng mô tả văn bản hoặc từ khóa** thường thất bại, bởi hầu hết các hình ảnh **không được mô tả đầy đủ trong văn bản xung quanh**, vì hai lý do chính:

1. **Khó hoặc không thể** liệt kê rõ ràng tất cả các khía cạnh của một hình ảnh, do **số lượng cách diễn giải tiềm năng là vô hạn**.
2. **Không cần thiết phải mô tả toàn bộ**, bởi hầu hết thông tin có thể được người xem **hiểu ngay khi nhìn vào hình ảnh**.

Vì vậy, phần mô tả bằng văn bản thường chỉ tập trung vào **siêu dữ liệu không được mã hóa trực tiếp trong hình ảnh**, chẳng hạn như **tác giả** của nó.  
Ví dụ, hình ảnh trong **Hình 1** có thể chỉ được mô tả là *bản sao chụp của bức tranh “The Dream Window in the Old Liselund Castle” của Georg Achen*. Điều này sẽ **ngăn cản** hình ảnh được tìm thấy bởi những người đang tìm kiếm:

- Hình ảnh **người phụ nữ nhìn ra cửa sổ**,
- Hình ảnh thể hiện **hoạt động “mơ mộng”** (*daydreaming*),
- Hoặc hình ảnh có **bầu không khí u sầu**.

Do đó, cách tự nhiên, trực tiếp và **biểu đạt nhất** để tìm hình ảnh với một nội dung cụ thể — vốn có thể **phức tạp và khó định nghĩa** — là **sử dụng một hình ảnh mẫu làm truy vấn** thay vì từ khóa văn bản. Cách tiếp cận này được gọi là **truy xuất hình ảnh dựa trên nội dung (Content-Based Image Retrieval - CBIR)** [49] và đã trở thành một lĩnh vực nghiên cứu sôi nổi từ năm 1992 [31,36].

> “Hình ảnh phải được **nhìn nhận và tìm kiếm như chính nó**.”  
> — *Smeulders et al., 2000* [49], trong bài khảo sát toàn diện về **CBIR giai đoạn đầu**.
**Hình 2** minh họa ví dụ về ba tập hình ảnh khác nhau được truy xuất dựa trên cùng một ảnh truy vấn, nhưng phụ thuộc vào **loại nhiệm vụ CBIR**.

Trong hai thập kỷ kể từ khi Smeulders et al. công bố bài khảo sát năm 2000, lĩnh vực **truy xuất hình ảnh dựa trên nội dung (CBIR)** đã trải qua **ít nhất hai cuộc cách mạng lớn** (sẽ được trình bày chi tiết trong **Mục 2**). Tuy nhiên, hầu hết **các thách thức chính và hướng nghiên cứu trọng tâm** đã được xác định từ thời điểm đó.

Một trong những thách thức lớn này là **khoảng cách ngữ nghĩa (semantic gap)**, được Smeulders et al. định nghĩa như sau:

> *“Khoảng cách ngữ nghĩa là sự thiếu trùng khớp giữa thông tin có thể được trích xuất từ dữ liệu hình ảnh và sự diễn giải mà người dùng gán cho dữ liệu đó trong một tình huống cụ thể.”*  
> — [49, Mục 2.4]

Diễn giải theo cách nói của **de Saint-Exupéry**, **khoảng cách ngữ nghĩa** là **sự khác biệt giữa việc nhìn nhận một hình ảnh bằng đôi mắt** — khách quan, chỉ như là tập hợp các **đối tượng, hình dạng, kết cấu** — và **nhìn nhận bằng trái tim**, chủ quan hơn, bao gồm **kiến thức về thế giới, cảm xúc và các yếu tố tiềm ẩn “giữa các điểm ảnh”**.

---

## Mức độ trừu tượng và khoảng cách ngữ nghĩa
Kích thước của khoảng cách ngữ nghĩa phụ thuộc vào **mức độ trừu tượng** của mục tiêu tìm kiếm mà người dùng đang theo đuổi.  
Smeulders et al. [49] định nghĩa mức độ này **trên một thang liên tục** từ **miền hẹp** đến **miền rộng**.  

Điều này có thể được minh họa thông qua **ba loại nhiệm vụ CBIR phổ biến nhất hiện nay**, được trình bày trong **Hình 2**:

### 1. **Duplicate Retrieval (Truy xuất trùng lặp)**
Tìm kiếm các hình ảnh **có nội dung hoàn toàn giống nhau**, nhưng có thể đã được **xử lý hậu kỳ** khác nhau, ví dụ:
- Cắt xén (cropping),
- Thay đổi kích thước (scaling),
- Điều chỉnh màu sắc, độ sáng, độ tương phản, v.v.

Đây là **loại nhiệm vụ đơn giản nhất**, vì nội dung hình ảnh không thay đổi về mặt ngữ nghĩa, chỉ khác nhau về hình thức.

### 2. **Instance Retrieval (Truy xuất thể hiện)**
Tìm kiếm hình ảnh **miêu tả cùng một thể hiện cụ thể của một đối tượng**, ví dụ:
- Một **người cụ thể**,
- Một **tòa nhà cụ thể**.

Đây là **nhiệm vụ được nghiên cứu nhiều nhất** trong CBIR nhờ:
- Bản chất **rõ ràng, có ground-truth chính xác** nhưng không quá tầm thường.
- Có nhiều **bộ dữ liệu chuẩn hóa** để đánh giá, ví dụ: [28,39,40,43].

Trong những năm gần đây, đã có **những bước tiến lớn** trong lĩnh vực này (sẽ được trình bày chi tiết trong **Mục 2**).

### 3. **Semantic Retrieval (Truy xuất ngữ nghĩa)**
Bao phủ **phần còn lại của phổ nhiệm vụ** — rộng hơn instance retrieval, mục tiêu là **tìm các hình ảnh thuộc cùng một "danh mục" (category)** với ảnh truy vấn.

> **Lưu ý:** Trong bối cảnh này, *category* **không nhất thiết** phải là lớp đối tượng (object class).

Thực tế:
- **Tập hợp các category chỉ bị giới hạn bởi trí tưởng tượng của người dùng.**
- **Một hình ảnh thường thuộc về rất nhiều category cùng lúc** (xem lại **Hình 1**).

Vì vậy:
- **Mục tiêu tìm kiếm chính xác** thường **không thể xác định** chỉ dựa trên ảnh truy vấn.
- Và **gần như chắc chắn sẽ khác nhau giữa các người dùng**, ngay cả khi truy vấn là giống nhau.

Do đó, **các phương pháp giải quyết vấn đề này** thường phải **tương tác với người dùng** để **điều chỉnh thước đo độ tương đồng** của hệ thống sao cho **phù hợp với suy nghĩ của người dùng** [55,12,15,5,7].

---

## Ý nghĩa và tầm quan trọng của việc học biểu diễn hình ảnh
Việc **học các biểu diễn hình ảnh có ý nghĩa**, có thể nắm bắt được:
- Các **phân biệt ngữ nghĩa tinh tế**,
- Và **các khía cạnh đa dạng của ý nghĩa hình ảnh**,

là **vô cùng quan trọng**.  
Tuy nhiên:
- **Truy xuất ngữ nghĩa** vẫn nhận được **ít sự quan tâm hơn** so với **truy xuất thể hiện**.
- Nguyên nhân chính:
  1. Khái niệm **“liên quan”** và **“tương đồng”** ít được định nghĩa rõ ràng.
  2. Thiếu **bộ dữ liệu benchmark phù hợp**.

Trong bài nghiên cứu này:
- Chúng tôi sẽ **đánh giá các phương pháp mới nhất cho truy xuất ngữ nghĩa** (xem **Mục 4**),
- Và **đánh giá trạng thái hiện tại của khoảng cách ngữ nghĩa**, **20 năm** sau giai đoạn "CBIR sơ khai".

---

## Mối quan hệ giữa ba loại nhiệm vụ CBIR
- **Duplicate Retrieval** nằm **ở đầu phổ nhiệm vụ** (miền hẹp nhất).  
  - **Khoảng cách ngữ nghĩa gần như không tồn tại.**
  - Chỉ cần định nghĩa **danh sách các bất biến** (invariances) như xoay, cắt xén, v.v.

- **Instance Retrieval** khó hơn Duplicate Retrieval,  
  - Nhưng vẫn có thể **giải quyết bằng cách đối chiếu các mẫu trực quan chi tiết và bố cục hình học**.

- **Semantic Retrieval** có **miền rộng nhất**,  
  - Yêu cầu nhiều hơn chỉ là so khớp mẫu hình ảnh.

Theo **Smeulders et al. [49]**, để **thu hẹp khoảng cách ngữ nghĩa**, cần **tích hợp các nguồn thông tin ngữ nghĩa bên ngoài hình ảnh**.

---

## Tóm tắt hướng nghiên cứu
- **Mục 2:** Tóm tắt các tiến bộ trong Instance Retrieval hai thập kỷ qua.  
- **Mục 3:** Trình bày hạn chế của các kỹ thuật hiện tại khi áp dụng vào Semantic Retrieval.  
- **Mục 4:** Xem xét các hướng tích hợp thông tin ngữ nghĩa bên ngoài hình ảnh.  
- **Mục 5:** Thảo luận những yếu tố còn thiếu để **tiến xa hơn trong CBIR miền rộng**.

## 2. Sự phát triển của Instance Retrieval

Trong giai đoạn từ năm 2000 đến 2020, CBIR — với trọng tâm đặc biệt là *instance retrieval* — đã trải qua hai lần chuyển đổi lớn:  
- Lần đầu tiên bắt đầu vào năm 2003 [48], được khởi xướng bởi việc thích ứng và cải tiến các kỹ thuật từ lĩnh vực tìm kiếm văn bản.  
- Làn sóng đột phá thứ hai bắt nguồn từ việc ứng dụng các phương pháp học sâu vào CBIR, bắt đầu từ năm 2014 [4,45].  

Chúng tôi sẽ phác thảo các cột mốc chính của hai giai đoạn đổi mới này trong phần sau.

---

### 2.1 Đặc trưng thủ công và Từ trực quan (Visual Words)

#### Đặc trưng cục bộ như Từ trực quan
Năm 2003, Sivic và Zisserman [48] tìm cách phát hiện sự xuất hiện của một đối tượng cụ thể trong video và, để làm điều này, họ đã thích ứng mô hình mô tả tài liệu **bag-of-words (BoW)** — vốn phổ biến trong lĩnh vực tìm kiếm văn bản — sang bài toán tìm kiếm ảnh.  

- **Tương tự như văn bản**:  
  - "Từ" được thay bằng **đặc trưng cục bộ** tại các điểm nổi bật (keypoints) trong ảnh.  
  - Các đặc trưng này được **lượng tử hóa** thành một **từ vựng “từ trực quan”** bằng thuật toán **k-Means**.  
  - Giống như văn bản, số lần xuất hiện của các từ trực quan trong ảnh được đếm và tổng hợp thành một **vector tf-idf** biểu diễn toàn bộ hình ảnh.  
  - Vì khoảng cách Euclidean không có ý nghĩa trong không gian chiều cao, nên **độ tương đồng cosin** được sử dụng để đo độ giống nhau giữa hai ảnh.  

Quy trình này minh họa khung tổng quát cho việc trích xuất biểu diễn ảnh, được sử dụng trong CBIR từ thời điểm đó cho đến nay [30]:  
1. **Bộ trích xuất đặc trưng cục bộ** tính toán đặc trưng tại các keypoints trong ảnh.  
2. Các đặc trưng cục bộ này sau đó được ánh xạ (embedding) sang một không gian khác, chẳng hạn như chỉ số của từ trực quan.  
3. Cuối cùng, chúng được tổng hợp thành **biểu diễn toàn cục** (global representation).  

Biểu diễn toàn cục cho phép tìm kiếm hiệu quả một danh sách ban đầu các ảnh ứng viên. Ngoài ra, đặc trưng cục bộ thường được sử dụng để thực hiện **kiểm tra không gian (spatial verification)** và bước **xếp hạng lại (re-ranking)** cho các ứng viên xếp hạng cao nhằm loại bỏ những kết quả khớp sai [48,39].  
Kỹ thuật này khá đặc thù cho instance retrieval, vì nó so khớp các vector đặc trưng cục bộ giữa ảnh truy vấn và ảnh tìm được để xác minh rằng chúng có bố cục hình học tương ứng.

---

#### Hướng tới các phép nhúng phức tạp hơn
Các công trình sau đó trong giai đoạn này chủ yếu tập trung vào việc cải tiến bước **embedding và aggregation**, trong khi vẫn sử dụng cùng một bộ trích xuất đặc trưng cục bộ trong suốt một thập kỷ.  

- **Hessian-affine detector** [34] thường được sử dụng để tìm ra các keypoints, tức các điểm đặc trưng bất biến với phép biến đổi affine cũng như bền vững trước những thay đổi nhỏ về ánh sáng và góc nhìn.  
- Các keypoints này sau đó được mô tả bằng **SIFT** [33] hoặc **RootSIFT** [1].  
  - RootSIFT chỉ là một phép biến đổi đơn giản của SIFT: chuẩn hóa L1 vector SIFT rồi lấy căn bậc hai từng phần tử.  
  - Trong không gian kết quả, khoảng cách Euclidean giữa các vector RootSIFT tương ứng với một **kernel so khớp histogram** trong không gian SIFT.  

Trong nghiên cứu của Sivic và Zisserman [48], phép nhúng (embedding) sẽ biến mỗi vector đặc trưng cục bộ thành một vector chỉ mục one-hot của từ vựng với trọng số tf-idf.

---

#### Các phương pháp nhúng tiên tiến hơn

Việc gộp đặc trưng (aggregation) trong BoW ban đầu chỉ đơn giản là phép **cộng tổng**.  
Tuy nhiên, việc biểu diễn vector đặc trưng cục bộ chỉ bằng **một số nguyên** (chỉ số cụm) đã gây mất mát thông tin nghiêm trọng và không phản ánh đúng phân bố thực tế của các đặc trưng cục bộ.  
Việc **gán cứng (hard assignment)** một đặc trưng vào duy nhất một cụm cũng không bền vững trước những thay đổi nhỏ khi descriptor nằm gần ranh giới cụm.  

Để khắc phục, **Perronnin et al. [38]** đề xuất sử dụng **Fisher Vector** cho CBIR:  
- Dữ liệu huấn luyện được lượng tử hóa thành các từ trực quan bằng cách **fit một mô hình Gaussian Mixture Model (GMM)**.  
- Mỗi vector đặc trưng cục bộ được biến đổi thành **gradient của log-likelihood** đối với trung bình của các Gaussian.  
- Cách này thực hiện một **soft assignment có trọng số** đến các cụm, tạo ra descriptor đặc hơn, nhiều thông tin hơn, nhưng cũng có **chiều rất cao**.  
- Kết quả: chỉ với **1 từ trực quan**, Fisher Vector đã đạt hiệu năng tương đương với BoW có **4.000 từ**.  

---

Một cách đơn giản hơn nhưng hiệu quả ngang bằng (thậm chí vượt trội trong một số trường hợp) là **VLAD (Vectors of Locally Aggregated Descriptors)**, được đề xuất bởi **Jégou et al. [29]**:  
- VLAD vẫn dùng **hard assignment** (mỗi descriptor gán vào cụm gần nhất).  
- Tuy nhiên, thay vì chỉ lưu chỉ số cụm, VLAD ghi nhận **vector phần dư (residuals)** giữa descriptor và tâm cụm.  
- Cấu trúc vector nhúng: được chia thành **k đoạn**, với k là số cụm.  
  - Đoạn ứng với cụm gần nhất = hiệu giữa descriptor và tâm cụm.  
  - Các đoạn khác = 0.  
- Kích thước embedding = **(số cụm) × (kích thước descriptor cục bộ)**.  
- Sau khi cộng dồn tất cả vector phần dư, kết quả được **chuẩn hóa L2**, rồi **giảm chiều bằng PCA kèm whitening**, đưa descriptor toàn cục về mức có thể xử lý (vài trăm chiều).

---

VLAD, về định nghĩa, rất nhạy với **khoảng cách Euclidean** giữa descriptor và tâm cụm.  
Tuy nhiên, khoảng cách Euclidean trong không gian cao chiều thường **không còn ý nghĩa**.  
Trong một nghiên cứu tiếp theo, **Jégou và Zisserman [30]** đã đề xuất:  
- **Chuẩn hóa L2 các residuals**, để mã hóa **góc (angle)** thay vì độ lớn.  
- Cách này được gọi là **Triangulation Embedding**.  
- Vì khoảng cách Euclidean kém ý nghĩa, nên gán cứng (hard assignment) cũng không hợp lý.  
- Triangulation embedding thay vào đó mã hóa góc giữa descriptor và **tất cả các từ trực quan**.  
- Biểu diễn này sau đó được whitening và cho thấy **hiệu năng vượt trội hơn Fisher Vector và VLAD**.

---

Tuy vậy, **Husain và Bober [25]** chỉ ra rằng:  
- Việc so sánh mỗi descriptor với **tất cả từ trực quan** không thể mở rộng cho các tập dữ liệu lớn.  
- Gán mềm (soft assignment) thì thường không ổn định và trong thực tế thường suy thoái thành gán cứng.  

Để giải quyết, họ đề xuất một giải pháp trung gian gọi là **Robust Visual Descriptors (RVDs)**:  
- Gán descriptor vào **một vài cụm gần nhất** thay vì tất cả.  
- Trọng số dựa trên **thứ hạng (rank) trong số hàng xóm gần nhất**, chứ không dựa trên khoảng cách tuyệt đối.  
- Các descriptor này không được whitening toàn cục, mà **whitening riêng trên từng cụm**.  
- Kết quả: RVD có hiệu năng cạnh tranh với triangulation embedding, nhưng **tính toán nhanh hơn** và **bền vững hơn khi giảm chiều**.
### 2.2 Học sâu (Deep Learning)

#### Sự xuất hiện của đặc trưng học sâu
Năm 2014 đánh dấu một bước ngoặt lớn đối với CBIR khi **mạng nơ-ron tích chập (CNNs)** bắt đầu được áp dụng.  
- Thay vì dựa vào đặc trưng thủ công (SIFT, RootSIFT, v.v.), các nhà nghiên cứu khai thác **biểu diễn từ mạng học sâu** đã được huấn luyện cho các nhiệm vụ nhận dạng hình ảnh quy mô lớn.  
- Các đặc trưng từ những lớp trung gian của CNN cho thấy khả năng biểu diễn mạnh mẽ, bền vững trước biến đổi hình học và ngữ cảnh, vượt xa những gì đặc trưng thủ công đạt được.  

---

#### Biểu diễn toàn cục từ CNN
Một trong những cách tiếp cận đầu tiên là **trích xuất trực tiếp vector đặc trưng từ các lớp fully-connected** của CNN huấn luyện trên ImageNet.  
- Các vector này được dùng như **biểu diễn toàn cục** của ảnh.  
- Tuy nhiên, chúng thường **thiếu tính bất biến không gian**, do đó chưa phù hợp hoàn toàn với instance retrieval.  

Để cải thiện, các nghiên cứu tiếp theo tận dụng **đặc trưng từ các lớp tích chập (convolutional layers)**:  
- Các lớp này sinh ra **map đặc trưng không gian (spatial feature maps)** thay vì chỉ một vector cố định.  
- Từ đó, các kỹ thuật tổng hợp như:  
  - **Max pooling**  
  - **Sum pooling**  
  - **R-MAC (Regional Maximum Activations of Convolutions)**  
được đề xuất để tạo vector biểu diễn toàn cục có tính bất biến cao hơn.  

---

#### Học end-to-end cho Instance Retrieval
Sau giai đoạn tận dụng CNN huấn luyện sẵn, nghiên cứu chuyển sang hướng **huấn luyện end-to-end chuyên biệt cho retrieval**.  
- Mục tiêu: tối ưu trực tiếp không gian đặc trưng sao cho phù hợp với nhiệm vụ so khớp ảnh.  
- Các kỹ thuật then chốt:  
  - **Triplet loss** và **Contrastive loss**: buộc các ảnh giống nhau gần nhau hơn, ảnh khác nhau bị đẩy xa ra.  
  - **Fine-tuning** trên dữ liệu retrieval thay vì chỉ sử dụng dữ liệu phân loại.  

---

#### Học sâu kết hợp với xác minh không gian
Mặc dù CNN mang lại biểu diễn toàn cục mạnh mẽ, nhiều nghiên cứu vẫn kết hợp với **xác minh không gian (spatial verification)** và **re-ranking** dựa trên đặc trưng cục bộ để cải thiện độ chính xác cuối cùng.  

---

#### Tóm lại
Trong giai đoạn **2014–2020**, học sâu đã:  
- Thay thế gần như hoàn toàn đặc trưng thủ công.  
- Mang đến biểu diễn ảnh giàu ngữ nghĩa, bất biến tốt và hiệu quả hơn.  
- Mở ra các hệ thống **retrieval end-to-end** hiện đại, trong đó toàn bộ pipeline (từ trích xuất đặc trưng đến truy vấn) được mạng nơ-ron tối ưu trực tiếp.
### Vai trò của các tập dữ liệu  

Trong khi mô hình sử dụng **đặc trưng cục bộ được tổng hợp** cho CBIR đã có từ năm 2003 [48], thì nghiên cứu trong lĩnh vực này lại sôi động nhất trong giai đoạn **2010 đến 2016**. Một lý do có thể giải thích cho sự chậm trễ này là do **thiếu các tập dữ liệu chuẩn mực và phù hợp** để làm benchmark.  

Trong các năm **2007 và 2008**, ba tập dữ liệu **Oxford Buildings** [39], **Paris Buildings** [40], và **INRIA Holidays** [28] đã được công bố. Chúng nhanh chóng trở thành **benchmark tiêu chuẩn** cho bài toán truy hồi theo thực thể (instance retrieval) và tạo động lực mới cho lĩnh vực này, nhờ cung cấp một cơ sở rõ ràng để đánh giá và so sánh các phương pháp.  

- **Oxford Buildings** và **Paris Buildings**: bao gồm các bức ảnh khác nhau của nhiều công trình kiến trúc nổi tiếng ở Oxford và Paris, với sự đa dạng lớn về góc chụp, tỷ lệ và mức độ che khuất.  
- **Holidays dataset**: bao gồm bộ sưu tập ảnh du lịch cá nhân, trung bình có khoảng **3 góc nhìn khác nhau cho mỗi cảnh**.  

Mặc dù các tập dữ liệu này khá **thách thức**, nhưng nhiệm vụ truy hồi ảnh hiển thị **cùng một đối tượng hoặc cảnh** với truy vấn lại được **xác định rõ ràng với ground truth minh bạch**.  
Tuy nhiên, các tập dữ liệu này cũng bộc lộ một số hạn chế:  
- **Kích thước nhỏ**: mỗi tập chỉ chứa vài nghìn ảnh, dẫn đến nguy cơ **overfitting** khi huấn luyện các mô hình phức tạp.  
- **Phạm vi hạn chế**: chủ yếu tập trung vào ảnh công trình kiến trúc hoặc cảnh quan cụ thể, không bao quát được sự đa dạng của các đối tượng trong thế giới thực.  
- **Độ khó chưa đủ lớn**: mặc dù thách thức, nhưng quy mô và độ đa dạng chưa phản ánh đúng độ phức tạp của các hệ thống truy hồi hình ảnh hiện đại.  

Để giải quyết những vấn đề này, các tập dữ liệu **lớn hơn và đa dạng hơn** đã lần lượt ra đời:  
- **Google Landmarks Dataset (GLD)** [2018, 2019]: bao gồm hàng triệu hình ảnh của hàng trăm nghìn địa danh trên toàn thế giới, thu thập từ Internet. Đây là một bước tiến quan trọng, vì nó cho phép huấn luyện và đánh giá các mô hình học sâu ở **quy mô lớn thực sự**.  
- Các tập dữ liệu mở rộng khác được thiết kế nhằm thúc đẩy **học biểu diễn sâu (deep representation learning)** và tăng tính **khả chuyển (transferability)** của mô hình sang nhiều bối cảnh thực tế hơn.  

Sự ra đời của các tập dữ liệu này đã mở đường cho **làn sóng thứ hai của CBIR** dựa trên **deep learning** từ năm 2014, đồng thời tiếp tục đóng vai trò nền tảng cho những cải tiến mới trong giai đoạn gần đây.  
### 2.2 Các đặc trưng CNN được đào tạo sẵn

Sau hơn một thập kỷ thống trị mà không bị thách thức, vị thế của các đặc trưng cục bộ được tạo thủ công trong CBIR cuối cùng đã chứng kiến một sự thay đổi đáng kể trong cách biểu diễn hình ảnh nhờ sự trỗi dậy của học sâu.

Các nghiên cứu độc lập của Babenko và cộng sự [4] và Razavian và cộng sự [45] lần đầu tiên chứng minh rằng có thể đạt được kết quả tốt đáng kinh ngạc chỉ bằng cách trích xuất các bộ mô tả hình ảnh toàn cục, được gọi là **mã thần kinh**, từ lớp kết nối đầy đủ đầu tiên của một mạng CNN được đào tạo trước trên ImageNet [14]. Với sự đơn giản tối giản của phương pháp này, hầu như không đòi hỏi nỗ lực kỹ thuật so với việc phát hiện điểm đặc trưng then chốt, trích xuất các đặc trưng cục bộ và tổng hợp chúng, đây là một kết quả đáng chú ý.

Chỉ một năm sau, Babenko và Lempitsky [3] đã cải thiện đáng kể hiệu suất của phương pháp này bằng cách trích xuất các đặc trưng hình ảnh không phải từ một lớp kết nối đầy đủ mà từ lớp tích chập cuối cùng, lớp này vẫn giữ được độ phân giải không gian. Kết quả là một tập hợp các vectơ đặc trưng có thể được ánh xạ gần đúng đến các vùng khác nhau trong hình ảnh. Các vectơ này sau đó được tổng hợp (sum-pooled), chuẩn hóa L2, giảm chiều bằng PCA, và một lần nữa được chuẩn hóa L2, để tạo ra các bộ mô tả được đặt tên rõ ràng là **SPoC** (Sum-Pooled Convolutional features).

Trong những năm tiếp theo, nghiên cứu chủ yếu tập trung vào việc sử dụng các bộ trích xuất đặc trưng thần kinh được đào tạo sẵn và thiết kế các hàm tổng hợp tinh vi. Nhiều phương pháp trong số đó cố gắng tìm điểm chung giữa tổng hợp trung bình (sum pooling) và tổng hợp tối đa (max pooling), ví dụ: bằng cách lấy trung bình các kích hoạt trên một vài phản hồi hàng đầu như trong **PMP** (Partial Mean Pooling) [54] hoặc bằng cách nội suy trơn tru giữa hai thái cực như trong **GeM** (Generalized Mean Pooling) [42].

Tuy nhiên, các đặc trưng tích chập được tổng hợp có một nhược điểm: Không giống như các đặc trưng cục bộ truyền thống, chúng không cho phép định vị chính xác các đối tượng khớp nhau và do đó không tương thích với các kỹ thuật như xác minh không gian và xếp hạng lại, vốn phụ thuộc vào thông tin hình học.

Để đạt được mục tiêu này, Tolias và cộng sự [50] đã đề xuất phương pháp **R-MAC** (Regional Maximum Activation of Convolutions), một quy trình gồm hai bước:
- Bản đồ đặc trưng tích chập được chia thành các vùng chồng chéo với các kích thước khác nhau.
- Các vectơ đặc trưng cục bộ trong mỗi vùng được tổng hợp bằng phương pháp gộp tối đa (max-pooled).

Các vectơ đặc trưng cục bộ này (được gọi là vectơ MAC) sau đó được làm trắng (whitened) và được tổng hợp lại (sum-pooled) thành một bộ mô tả hình ảnh toàn cục duy nhất là R-MAC. Đối với việc xếp hạng lại không gian, độ tương đồng giữa vectơ MAC của ảnh truy vấn và các vectơ MAC vùng riêng lẻ của một số kết quả truy xuất hàng đầu có thể được sử dụng để định vị đối tượng trong truy vấn trong các ảnh được truy xuất và để tinh chỉnh thứ hạng kết quả.

Các kỹ thuật này đã đưa CBIR dựa trên các đặc trưng được trích xuất từ các CNN được đào tạo sẵn tiến một bước rất dài, mặc dù các bộ mô tả thủ công như **RVD** [25] vẫn có khả năng cạnh tranh với chúng trên các điểm chuẩn truy xuất theo thể loại (instance retrieval benchmarks).
### 2.3 Học từ đầu đến cuối cho Truy xuất Hình ảnh

Học sâu cuối cùng đã trở nên vượt trội không thể phủ nhận so với các kỹ thuật CBIR truyền thống dựa trên các đặc trưng thủ công khi các nhà nghiên cứu bắt đầu điều chỉnh CNN được sử dụng để trích xuất đặc trưng cho nhiệm vụ truy xuất hình ảnh thay vì sử dụng mạng được đào tạo sẵn. Chúng tôi coi sự thay đổi trọng tâm từ chuyển đổi và tổng hợp đặc trưng sang học đặc trưng thực tế là sự thay đổi mô hình quan trọng thứ hai trong CBIR.

**Đặc trưng Toàn cục**  
Gordo và cộng sự [20] là một trong những người đầu tiên thành công trong nỗ lực này và thiết lập trạng thái nghệ thuật trong truy xuất thể loại ít nhất hai năm. Họ xây dựng dựa trên R-MAC [50] và triển khai nó dưới dạng các lớp khả vi trên kiến trúc CNN VGG16, sau đó có thể được đào tạo từ đầu đến cuối. Để làm được điều này, họ sử dụng hàm mất mát triplet [47], một mục tiêu đào tạo từ lĩnh vực học metric sâu. Bằng cách đào tạo trên một tập dữ liệu được tuyển chọn gồm các địa danh nổi tiếng, họ học được một biểu diễn đặc trưng trong đó các hình ảnh của cùng một địa danh ở gần nhau hơn một ngưỡng nhất định so với hai hình ảnh của các địa danh khác nhau, điều này hỗ trợ mục tiêu của truy xuất thể loại.

Phương pháp này sau đó đã được mở rộng bằng cách trích xuất các đặc trưng R-MAC từ nhiều lớp của CNN và trọng số các đặc trưng riêng lẻ của mỗi vùng bằng độ phân kỳ Kullback-Leibler giữa các phân phối của khoảng cách Euclid giữa các bộ mô tả khớp và không khớp, sao cho các đặc trưng vùng phân biệt hơn có trọng số cao hơn [26]. Động lực để kết hợp các đặc trưng từ nhiều lớp nằm ở các mức độ trừu tượng hóa hình ảnh khác nhau: các đặc trưng từ các lớp đầu tiên chỉ ra nhiều hơn các thuộc tính hình ảnh, trong khi các lớp sau cung cấp một biểu diễn trừu tượng hơn về mặt ngữ nghĩa.

Trái ngược với hàm mất mát triplet, Radenović và cộng sự [42] nhận thấy hàm mất mát tương phản mang lại hiệu suất cuối cùng tốt hơn, đồng thời chỉ yêu cầu các cặp thay vì bộ ba hình ảnh để đào tạo. Quan trọng hơn, họ đề xuất một kỹ thuật không giám sát để tạo dữ liệu đào tạo gồm các cặp hình ảnh khớp và không khớp cho truy xuất thể loại mà không cần chú thích của con người: Các hình ảnh trong tập dữ liệu đào tạo được phân cụm dựa trên biểu diễn BoW của chúng bằng cách sử dụng các đặc trưng RootSIFT cục bộ và xác minh không gian được áp dụng để đảm bảo rằng tất cả các hình ảnh trong một cụm hiển thị cùng một đối tượng. Sau đó, một mô hình 3-D được xây dựng cho mỗi cụm bằng cách sử dụng kỹ thuật structure-from-motion (SfM), để từ các mô hình này có thể xác định được hai hình ảnh có mô tả cùng một đối tượng hay không. Điều này cũng cho phép các hình ảnh của cùng một địa danh nhưng được chụp từ các góc nhìn khác nhau và rời rạc được coi là không khớp. Thông tin về vị trí máy ảnh thu được từ SfM hơn nữa cho phép khai thác các cặp hình ảnh tích cực đầy thách thức có lượng chồng chéo đáng kể.

Các phương pháp học metric này đã dẫn đến một sự cải thiện ấn tượng về hiệu suất truy xuất thể loại về độ chính xác trung bình (AP), mặc dù chúng không tối ưu hóa nó trực tiếp mà là một mục tiêu ủy quyền dựa trên khoảng cách trong không gian đặc trưng đã học. Vì AP là số liệu quan trọng nhất để đánh giá các phương pháp truy xuất, nên việc tối ưu hóa nó trực tiếp thay vì một tác vụ ủy quyền có vẻ mong muốn. Tuy nhiên, điều đó đòi hỏi phải tính đến không chỉ một mẫu đơn lẻ, một cặp hoặc một bộ ba như trước đây, mà toàn bộ danh sách kết quả được xếp hạng. Một lợi ích rõ ràng là các mục tiêu theo danh sách như vậy nhạy cảm với vị trí: Tác động của một cặp hoặc bộ ba đơn lẻ liên quan đến các hình ảnh ở đầu bảng xếp hạng nên cao hơn so với cuối danh sách. Tuy nhiên, độ chính xác trung bình không khả vi, vì nó liên quan đến việc sắp xếp các hình ảnh theo độ tương đồng với truy vấn. Để có thể tối ưu hóa AP trong bối cảnh học từ đầu đến cuối, He và cộng sự [22] đã đề xuất một phép xấp xỉ khả vi của AP bằng cách sử dụng phân bin histogram, phương pháp này đã được Revaud và cộng sự [46] áp dụng cho CBIR và cải thiện trạng thái nghệ thuật. Vì độ tương đồng cosine, thường được sử dụng để truy xuất, bị giới hạn trong [-1, 1], nên phạm vi của các điểm tương đồng có thể dễ dàng được chia thành một số lượng cố định các bin có kích thước bằng nhau. Sau đó, các hình ảnh được gán mềm vào các bin có trung tâm gần nhất với điểm truy xuất của hình ảnh để thu được histogram của số lượng khớp tích cực và tiêu cực trong mỗi bin. Thay vì tính toán độ chính xác và độ nhớ lại cho mỗi vị trí có thể có trong bảng xếp hạng, các số liệu này giờ đây có thể được tính toán cho mỗi bin và kết hợp để xấp xỉ AP.

Tuy nhiên, việc lượng tử hóa điểm tương đồng thành các bin bỏ qua các biến thể của thứ hạng trong mỗi bin, điều này có thể có tác động đặc biệt lớn đến AP ở các vị trí đầu của bảng xếp hạng. Nhược điểm này gần đây đã được khắc phục bằng một cách tiếp cận khác để xấp xỉ AP: Thay vì sắp xếp lượng tử hóa bằng phân bin, chính thao tác sắp xếp được làm mềm bằng cách thay thế hàm bước Heaviside chỉ ra liệu một phần tử của danh sách có đứng trước phần tử khác hay không bằng một hàm sigmoid để tránh tiêu biến gradient [41]. Điều này cho phép sắp xếp khả vi và tính toán một phiên bản làm mịn của AP, được gọi là Smooth-AP [9].

Với các phương pháp tiếp cận theo danh sách này, các biểu diễn toàn cục cho CBIR cuối cùng có thể được học từ đầu đến cuối mà không cần các bước trung gian thủ công hoặc các mục tiêu ủy quyền.

**Đặc trưng Cục bộ**  
Trong khi các bộ mô tả hình ảnh toàn cục thuận tiện cho các ứng dụng truy xuất, chúng không mạnh mẽ khi có sự che khuất hoặc nhiễu nền cũng như không phù hợp để xác minh không gian, một kỹ thuật quan trọng cho truy xuất thể loại. Do đó, các công trình khác nhằm mục đích học các bộ phát hiện và mô tả đặc trưng cục bộ một cách từ đầu đến cuối.

Ví dụ, Deep Local Features (DELF) [37] sử dụng các đặc trưng vùng thô được trích xuất từ một lớp tích chập của CNN được đào tạo trước và sau đó đào tạo một CNN nhỏ khác để đánh giá tầm quan trọng của các điểm đặc trưng được lấy mẫu dày đặc này. Để đào tạo, các trọng số dự đoán này được sử dụng để tổng hợp có trọng số (weighted sum pooling) các bộ mô tả cục bộ thành một vectơ đặc trưng toàn cục, cho phép tinh chỉnh các đặc trưng cục bộ bằng cách sử dụng giám sát ở cấp độ hình ảnh.

Hầu hết các hệ thống truy xuất thể loại sử dụng đặc trưng cục bộ áp dụng phương pháp tiếp cận hai giai đoạn: Đầu tiên, một tập hợp các hình ảnh ứng viên được truy xuất bằng cách so sánh các đặc trưng toàn cục và sau đó được xếp hạng lại bằng cách sử dụng xác minh không gian dựa trên các đặc trưng cục bộ. Cao và cộng sự [10] đã thống nhất việc học cả hai loại đặc trưng thành một mô hình duy nhất với hai nhánh: Một nhánh tổng hợp tất cả các vectơ đặc trưng của lớp tích chập cuối cùng của CNN thành các vectơ đặc trưng toàn cục và được đào tạo với một hàm mất mát học metric. Nhánh còn lại học một mô-đun chú ý để xác định các đặc trưng cục bộ đặc trưng và được đào tạo bằng cách sử dụng cross-entropy phân loại.

**Nhu cầu về các Điểm chuẩn Đầy thách thức hơn**  
Bên cạnh nhiều năng lực tính toán, các kỹ thuật học sâu đòi hỏi một điều quan trọng nhất: dữ liệu. Các tập dữ liệu truy xuất thể loại hiện có quá nhỏ để đào tạo các mạng nơ-ron sâu, do đó Babenko và cộng sự [4] đã tạo ra một tập dữ liệu địa danh mới với hơn 200.000 hình ảnh cho mục đích đào tạo, tập dữ liệu này sau đó cũng được sử dụng bởi các công trình khác về truy xuất hình ảnh sâu [20]. Ngày nay, tập dữ liệu quy mô lớn Google-Landmarks [37] được đề xuất vào năm 2017 thường được sử dụng để đào tạo. Nó bao gồm hơn một triệu hình ảnh của 12.894 địa danh từ khắp nơi trên thế giới.

Các tập dữ liệu này lớn hơn nhiều lần so với tập dữ liệu Oxford và Paris Buildings, nhưng các tập dữ liệu sau vẫn có liên quan để đánh giá và so sánh các phương pháp mới. Tuy nhiên, những tiến bộ nhanh chóng trong học sâu cho CBIR nhanh chóng dẫn đến sự bão hòa hiệu suất trên các điểm chuẩn này. Do đó, Radenović và cộng sự [43] đã xem xét lại hai tập dữ liệu này vào năm 2018 bằng cách cải thiện các chú thích ground-truth, tìm kiếm các truy vấn khó hơn, thêm các hình ảnh đánh lạc hướng đầy thách thức và xác định ba giao thức đánh giá khác nhau với các mức độ khó khác nhau.

Những phát triển này chứng minh tầm quan trọng của các tập dữ liệu đào tạo và điểm chuẩn phù hợp cho sự tiến bộ của truy xuất hình ảnh dựa trên nội dung.