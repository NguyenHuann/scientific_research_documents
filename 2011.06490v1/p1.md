# Truy xuất hình ảnh dựa trên nội dung và khoảng cách ngữ nghĩa trong kỷ nguyên học sâu

**Björn Barz và Joachim Denzler**  
Nhóm Thị giác Máy tính, Đại học Friedrich Schiller Jena, Jena, Đức  
{bjoern.barz, joachim.denzler}@uni-jena.de

## Tóm tắt
Truy xuất hình ảnh dựa trên nội dung đã đạt được những tiến bộ đáng kinh ngạc trong thập kỷ qua, đặc biệt là đối với nhiệm vụ truy xuất hình ảnh của cùng một đối tượng được thể hiện trong ảnh truy vấn. Kịch bản này được gọi là truy xuất thể hiện hoặc truy xuất đối tượng và yêu cầu đối chiếu các mẫu hình ảnh chi tiết giữa các hình ảnh. Tuy nhiên, yếu tố ngữ nghĩa không đóng vai trò quan trọng.

Điều này đặt ra câu hỏi: Liệu những tiến bộ gần đây trong truy xuất thể hiện có thể chuyển giao sang các kịch bản truy xuất hình ảnh tổng quát hơn không?

Để trả lời câu hỏi này, trước tiên chúng tôi cung cấp một cái nhìn tổng quan ngắn gọn về những cột mốc quan trọng nhất của truy xuất thể hiện. Sau đó, chúng tôi áp dụng các phương pháp này vào một nhiệm vụ truy xuất hình ảnh ngữ nghĩa và nhận thấy rằng chúng hoạt động kém hiệu quả hơn nhiều so với các phương pháp ít phức tạp và mang tính tổng quát hơn trong bối cảnh yêu cầu hiểu hình ảnh. Tiếp theo, chúng tôi xem xét các phương pháp hiện có nhằm thu hẹp cái gọi là "khoảng cách ngữ nghĩa" bằng cách tích hợp kiến thức thế giới có sẵn.

Chúng tôi kết luận rằng vấn đề then chốt đối với sự tiến bộ hơn nữa của truy xuất hình ảnh ngữ nghĩa nằm ở việc thiếu một định nghĩa nhiệm vụ tiêu chuẩn hóa và bộ dữ liệu đánh giá thích hợp.

**Từ khóa:** Truy xuất hình ảnh dựa trên nội dung · Truy xuất thể hiện · Truy xuất đối tượng · Truy xuất hình ảnh ngữ nghĩa · Khoảng cách ngữ nghĩa
## Giới thiệu

*"Người ta chỉ có thể nhìn rõ bằng trái tim. Điều cốt yếu thì vô hình đối với đôi mắt."*  
Câu nói nổi tiếng này của nhà văn Pháp **Antoine de Saint-Exupéry** áp dụng không chỉ cho cuộc sống mà còn cho cả lĩnh vực **thị giác máy tính**. Nhận thức của con người về hình ảnh vượt xa bề mặt thị giác bao gồm các điểm ảnh, màu sắc và các đối tượng. Ý nghĩa của một hình ảnh không thể chỉ được mô tả đơn giản bằng cách liệt kê tất cả các đối tượng có trong đó và xác định bố cục không gian của chúng.  

Chúng ta, với tư cách là con người, có khả năng nắm bắt vô vàn thông tin đa dạng và phức tạp chứa trong một hình ảnh chỉ trong nháy mắt, chẳng hạn như **các sự kiện đang xảy ra trong cảnh được thể hiện**, **các hoạt động mà con người đang thực hiện**, **mối quan hệ giữa họ**, **bầu không khí và tâm trạng của hình ảnh**, cũng như **cảm xúc mà nó truyền tải**. Nhiều khái niệm trong số này khó có thể mô tả bằng văn bản và thường được minh họa tốt nhất bằng cách cung cấp một hình ảnh ví dụ.

Ví dụ trong **Hình 1** minh họa sự đa dạng thông tin mà hình ảnh có thể truyền tải. Hình ảnh này có thể được mô tả từ nhiều góc độ khác nhau: **nội dung ngữ nghĩa**, **phong cách nghệ thuật**, **cảm xúc mà nó gợi lên cho người quan sát**, hoặc **thông tin siêu dữ liệu** về chính hình ảnh.  

Tùy thuộc vào **nền tảng** và **bối cảnh tình huống**, mỗi người quan sát sẽ nhận thức và diễn giải hình ảnh này theo một cách khác nhau. Do đó, việc **tìm kiếm hình ảnh trên web bằng mô tả văn bản hoặc từ khóa** thường thất bại, bởi hầu hết các hình ảnh **không được mô tả đầy đủ trong văn bản xung quanh**, vì hai lý do chính:

1. **Khó hoặc không thể** liệt kê rõ ràng tất cả các khía cạnh của một hình ảnh, do **số lượng cách diễn giải tiềm năng là vô hạn**.
2. **Không cần thiết phải mô tả toàn bộ**, bởi hầu hết thông tin có thể được người xem **hiểu ngay khi nhìn vào hình ảnh**.

Vì vậy, phần mô tả bằng văn bản thường chỉ tập trung vào **siêu dữ liệu không được mã hóa trực tiếp trong hình ảnh**, chẳng hạn như **tác giả** của nó.  
Ví dụ, hình ảnh trong **Hình 1** có thể chỉ được mô tả là *bản sao chụp của bức tranh “The Dream Window in the Old Liselund Castle” của Georg Achen*. Điều này sẽ **ngăn cản** hình ảnh được tìm thấy bởi những người đang tìm kiếm:

- Hình ảnh **người phụ nữ nhìn ra cửa sổ**,
- Hình ảnh thể hiện **hoạt động “mơ mộng”** (*daydreaming*),
- Hoặc hình ảnh có **bầu không khí u sầu**.

Do đó, cách tự nhiên, trực tiếp và **biểu đạt nhất** để tìm hình ảnh với một nội dung cụ thể — vốn có thể **phức tạp và khó định nghĩa** — là **sử dụng một hình ảnh mẫu làm truy vấn** thay vì từ khóa văn bản. Cách tiếp cận này được gọi là **truy xuất hình ảnh dựa trên nội dung (Content-Based Image Retrieval - CBIR)** [49] và đã trở thành một lĩnh vực nghiên cứu sôi nổi từ năm 1992 [31,36].

> “Hình ảnh phải được **nhìn nhận và tìm kiếm như chính nó**.”  
> — *Smeulders et al., 2000* [49], trong bài khảo sát toàn diện về **CBIR giai đoạn đầu**.
**Hình 2** minh họa ví dụ về ba tập hình ảnh khác nhau được truy xuất dựa trên cùng một ảnh truy vấn, nhưng phụ thuộc vào **loại nhiệm vụ CBIR**.

Trong hai thập kỷ kể từ khi Smeulders et al. công bố bài khảo sát năm 2000, lĩnh vực **truy xuất hình ảnh dựa trên nội dung (CBIR)** đã trải qua **ít nhất hai cuộc cách mạng lớn** (sẽ được trình bày chi tiết trong **Mục 2**). Tuy nhiên, hầu hết **các thách thức chính và hướng nghiên cứu trọng tâm** đã được xác định từ thời điểm đó.

Một trong những thách thức lớn này là **khoảng cách ngữ nghĩa (semantic gap)**, được Smeulders et al. định nghĩa như sau:

> *“Khoảng cách ngữ nghĩa là sự thiếu trùng khớp giữa thông tin có thể được trích xuất từ dữ liệu hình ảnh và sự diễn giải mà người dùng gán cho dữ liệu đó trong một tình huống cụ thể.”*  
> — [49, Mục 2.4]

Diễn giải theo cách nói của **de Saint-Exupéry**, **khoảng cách ngữ nghĩa** là **sự khác biệt giữa việc nhìn nhận một hình ảnh bằng đôi mắt** — khách quan, chỉ như là tập hợp các **đối tượng, hình dạng, kết cấu** — và **nhìn nhận bằng trái tim**, chủ quan hơn, bao gồm **kiến thức về thế giới, cảm xúc và các yếu tố tiềm ẩn “giữa các điểm ảnh”**.

---

## Mức độ trừu tượng và khoảng cách ngữ nghĩa
Kích thước của khoảng cách ngữ nghĩa phụ thuộc vào **mức độ trừu tượng** của mục tiêu tìm kiếm mà người dùng đang theo đuổi.  
Smeulders et al. [49] định nghĩa mức độ này **trên một thang liên tục** từ **miền hẹp** đến **miền rộng**.  

Điều này có thể được minh họa thông qua **ba loại nhiệm vụ CBIR phổ biến nhất hiện nay**, được trình bày trong **Hình 2**:

### 1. **Duplicate Retrieval (Truy xuất trùng lặp)**
Tìm kiếm các hình ảnh **có nội dung hoàn toàn giống nhau**, nhưng có thể đã được **xử lý hậu kỳ** khác nhau, ví dụ:
- Cắt xén (cropping),
- Thay đổi kích thước (scaling),
- Điều chỉnh màu sắc, độ sáng, độ tương phản, v.v.

Đây là **loại nhiệm vụ đơn giản nhất**, vì nội dung hình ảnh không thay đổi về mặt ngữ nghĩa, chỉ khác nhau về hình thức.

### 2. **Instance Retrieval (Truy xuất thể hiện)**
Tìm kiếm hình ảnh **miêu tả cùng một thể hiện cụ thể của một đối tượng**, ví dụ:
- Một **người cụ thể**,
- Một **tòa nhà cụ thể**.

Đây là **nhiệm vụ được nghiên cứu nhiều nhất** trong CBIR nhờ:
- Bản chất **rõ ràng, có ground-truth chính xác** nhưng không quá tầm thường.
- Có nhiều **bộ dữ liệu chuẩn hóa** để đánh giá, ví dụ: [28,39,40,43].

Trong những năm gần đây, đã có **những bước tiến lớn** trong lĩnh vực này (sẽ được trình bày chi tiết trong **Mục 2**).

### 3. **Semantic Retrieval (Truy xuất ngữ nghĩa)**
Bao phủ **phần còn lại của phổ nhiệm vụ** — rộng hơn instance retrieval, mục tiêu là **tìm các hình ảnh thuộc cùng một "danh mục" (category)** với ảnh truy vấn.

> **Lưu ý:** Trong bối cảnh này, *category* **không nhất thiết** phải là lớp đối tượng (object class).

Thực tế:
- **Tập hợp các category chỉ bị giới hạn bởi trí tưởng tượng của người dùng.**
- **Một hình ảnh thường thuộc về rất nhiều category cùng lúc** (xem lại **Hình 1**).

Vì vậy:
- **Mục tiêu tìm kiếm chính xác** thường **không thể xác định** chỉ dựa trên ảnh truy vấn.
- Và **gần như chắc chắn sẽ khác nhau giữa các người dùng**, ngay cả khi truy vấn là giống nhau.

Do đó, **các phương pháp giải quyết vấn đề này** thường phải **tương tác với người dùng** để **điều chỉnh thước đo độ tương đồng** của hệ thống sao cho **phù hợp với suy nghĩ của người dùng** [55,12,15,5,7].

---

## Ý nghĩa và tầm quan trọng của việc học biểu diễn hình ảnh
Việc **học các biểu diễn hình ảnh có ý nghĩa**, có thể nắm bắt được:
- Các **phân biệt ngữ nghĩa tinh tế**,
- Và **các khía cạnh đa dạng của ý nghĩa hình ảnh**,

là **vô cùng quan trọng**.  
Tuy nhiên:
- **Truy xuất ngữ nghĩa** vẫn nhận được **ít sự quan tâm hơn** so với **truy xuất thể hiện**.
- Nguyên nhân chính:
  1. Khái niệm **“liên quan”** và **“tương đồng”** ít được định nghĩa rõ ràng.
  2. Thiếu **bộ dữ liệu benchmark phù hợp**.

Trong bài nghiên cứu này:
- Chúng tôi sẽ **đánh giá các phương pháp mới nhất cho truy xuất ngữ nghĩa** (xem **Mục 4**),
- Và **đánh giá trạng thái hiện tại của khoảng cách ngữ nghĩa**, **20 năm** sau giai đoạn "CBIR sơ khai".

---

## Mối quan hệ giữa ba loại nhiệm vụ CBIR
- **Duplicate Retrieval** nằm **ở đầu phổ nhiệm vụ** (miền hẹp nhất).  
  - **Khoảng cách ngữ nghĩa gần như không tồn tại.**
  - Chỉ cần định nghĩa **danh sách các bất biến** (invariances) như xoay, cắt xén, v.v.

- **Instance Retrieval** khó hơn Duplicate Retrieval,  
  - Nhưng vẫn có thể **giải quyết bằng cách đối chiếu các mẫu trực quan chi tiết và bố cục hình học**.

- **Semantic Retrieval** có **miền rộng nhất**,  
  - Yêu cầu nhiều hơn chỉ là so khớp mẫu hình ảnh.

Theo **Smeulders et al. [49]**, để **thu hẹp khoảng cách ngữ nghĩa**, cần **tích hợp các nguồn thông tin ngữ nghĩa bên ngoài hình ảnh**.

---

## Tóm tắt hướng nghiên cứu
- **Mục 2:** Tóm tắt các tiến bộ trong Instance Retrieval hai thập kỷ qua.  
- **Mục 3:** Trình bày hạn chế của các kỹ thuật hiện tại khi áp dụng vào Semantic Retrieval.  
- **Mục 4:** Xem xét các hướng tích hợp thông tin ngữ nghĩa bên ngoài hình ảnh.  
- **Mục 5:** Thảo luận những yếu tố còn thiếu để **tiến xa hơn trong CBIR miền rộng**.
## 2. Sự phát triển của Instance Retrieval

Trong giai đoạn từ năm 2000 đến 2020, CBIR — với trọng tâm đặc biệt là *instance retrieval* — đã trải qua hai lần chuyển đổi lớn:  
- Lần đầu tiên bắt đầu vào năm 2003 [48], được khởi xướng bởi việc thích ứng và cải tiến các kỹ thuật từ lĩnh vực tìm kiếm văn bản.  
- Làn sóng đột phá thứ hai bắt nguồn từ việc ứng dụng các phương pháp học sâu vào CBIR, bắt đầu từ năm 2014 [4,45].  

Chúng tôi sẽ phác thảo các cột mốc chính của hai giai đoạn đổi mới này trong phần sau.

---

### 2.1 Đặc trưng thủ công và Từ trực quan (Visual Words)

#### Đặc trưng cục bộ như Từ trực quan
Năm 2003, Sivic và Zisserman [48] tìm cách phát hiện sự xuất hiện của một đối tượng cụ thể trong video và, để làm điều này, họ đã thích ứng mô hình mô tả tài liệu **bag-of-words (BoW)** — vốn phổ biến trong lĩnh vực tìm kiếm văn bản — sang bài toán tìm kiếm ảnh.  

- **Tương tự như văn bản**:  
  - "Từ" được thay bằng **đặc trưng cục bộ** tại các điểm nổi bật (keypoints) trong ảnh.  
  - Các đặc trưng này được **lượng tử hóa** thành một **từ vựng “từ trực quan”** bằng thuật toán **k-Means**.  
  - Giống như văn bản, số lần xuất hiện của các từ trực quan trong ảnh được đếm và tổng hợp thành một **vector tf-idf** biểu diễn toàn bộ hình ảnh.  
  - Vì khoảng cách Euclidean không có ý nghĩa trong không gian chiều cao, nên **độ tương đồng cosin** được sử dụng để đo độ giống nhau giữa hai ảnh.  

Quy trình này minh họa khung tổng quát cho việc trích xuất biểu diễn ảnh, được sử dụng trong CBIR từ thời điểm đó cho đến nay [30]:  
1. **Bộ trích xuất đặc trưng cục bộ** tính toán đặc trưng tại các keypoints trong ảnh.  
2. Các đặc trưng cục bộ này sau đó được ánh xạ (embedding) sang một không gian khác, chẳng hạn như chỉ số của từ trực quan.  
3. Cuối cùng, chúng được tổng hợp thành **biểu diễn toàn cục** (global representation).  

Biểu diễn toàn cục cho phép tìm kiếm hiệu quả một danh sách ban đầu các ảnh ứng viên. Ngoài ra, đặc trưng cục bộ thường được sử dụng để thực hiện **kiểm tra không gian (spatial verification)** và bước **xếp hạng lại (re-ranking)** cho các ứng viên xếp hạng cao nhằm loại bỏ những kết quả khớp sai [48,39].  
Kỹ thuật này khá đặc thù cho instance retrieval, vì nó so khớp các vector đặc trưng cục bộ giữa ảnh truy vấn và ảnh tìm được để xác minh rằng chúng có bố cục hình học tương ứng.

---

#### Hướng tới các phép nhúng phức tạp hơn
Các công trình sau đó trong giai đoạn này chủ yếu tập trung vào việc cải tiến bước **embedding và aggregation**, trong khi vẫn sử dụng cùng một bộ trích xuất đặc trưng cục bộ trong suốt một thập kỷ.  

- **Hessian-affine detector** [34] thường được sử dụng để tìm ra các keypoints, tức các điểm đặc trưng bất biến với phép biến đổi affine cũng như bền vững trước những thay đổi nhỏ về ánh sáng và góc nhìn.  
- Các keypoints này sau đó được mô tả bằng **SIFT** [33] hoặc **RootSIFT** [1].  
  - RootSIFT chỉ là một phép biến đổi đơn giản của SIFT: chuẩn hóa L1 vector SIFT rồi lấy căn bậc hai từng phần tử.  
  - Trong không gian kết quả, khoảng cách Euclidean giữa các vector RootSIFT tương ứng với một **kernel so khớp histogram** trong không gian SIFT.  

Trong nghiên cứu của Sivic và Zisserman [48], phép nhúng (embedding) sẽ biến mỗi vector đặc trưng cục bộ thành một vector chỉ mục one-hot của từ vựng với trọng số tf-idf.
### 2.2 Học sâu (Deep Learning)

#### Sự xuất hiện của đặc trưng học sâu
Năm 2014 đánh dấu một bước ngoặt lớn đối với CBIR khi **mạng nơ-ron tích chập (CNNs)** bắt đầu được áp dụng.  
- Thay vì dựa vào đặc trưng thủ công (SIFT, RootSIFT, v.v.), các nhà nghiên cứu sử dụng **biểu diễn từ mạng học sâu** đã được huấn luyện cho các nhiệm vụ nhận dạng hình ảnh quy mô lớn.  
- Các đặc trưng từ các lớp trung gian của CNN cho thấy khả năng biểu diễn mạnh mẽ, bền vững trước biến đổi hình học và ngữ cảnh, vượt xa những gì đặc trưng thủ công đạt được.

#### Biểu diễn toàn cục từ CNN
Một trong những cách tiếp cận ban đầu là **trích xuất trực tiếp vector đặc trưng từ các lớp fully-connected** của mạng được huấn luyện trên ImageNet.  
- Các vector này có thể sử dụng làm biểu diễn toàn cục cho hình ảnh.  
- Tuy nhiên, chúng thường thiếu tính bất biến không gian, do đó chưa lý tưởng cho instance retrieval.  

Để khắc phục, các phương pháp tiếp theo tận dụng **đặc trưng từ các lớp tích chập (convolutional layers)**:  
- Thay vì một vector cố định, các lớp này sinh ra **map đặc trưng không gian (spatial feature maps)**.  
- Từ đó, các kỹ thuật tổng hợp như **Max pooling, Sum pooling, R-MAC (Regional Maximum Activations of Convolutions)** đã được đề xuất để tạo ra vector biểu diễn toàn cục có tính bất biến cao hơn.

#### Học end-to-end cho Instance Retrieval
Sau giai đoạn khai thác CNN huấn luyện sẵn, nghiên cứu bắt đầu hướng đến **huấn luyện end-to-end** đặc thù cho instance retrieval.  
- Mục tiêu: tối ưu trực tiếp đặc trưng sao cho phù hợp với truy vấn và so khớp hình ảnh.  
- Các kỹ thuật quan trọng:  
  - **Triplet loss, Contrastive loss**: ép các ảnh giống nhau phải gần nhau trong không gian đặc trưng, và đẩy các ảnh khác xa ra.  
  - **Fine-tuning** trên dữ liệu retrieval thay vì chỉ dùng dữ liệu phân loại.  

#### Học sâu kết hợp với xác minh không gian
Mặc dù CNN cho ra biểu diễn toàn cục mạnh mẽ, nhưng nhiều nghiên cứu vẫn kết hợp với **xác minh không gian (spatial verification)** và **re-ranking** dựa trên đặc trưng cục bộ để cải thiện độ chính xác.  

#### Tóm lại
Trong giai đoạn 2014–2020, học sâu đã:  
- Thay thế gần như hoàn toàn đặc trưng thủ công.  
- Cho phép biểu diễn ảnh giàu ngữ nghĩa, bất biến tốt, và hiệu quả hơn.  
- Mở đường cho các hệ thống **retrieval end-to-end** hiện đại, nơi cả pipeline (từ trích xuất đặc trưng đến truy vấn) đều do mạng nơ-ron tối ưu trực tiếp.
